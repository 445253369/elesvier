{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding IMMUNITY now!\n",
      "https://www.sciencedirect.com/journal/10747613/year/2020/issues\n",
      "['https://www.sciencedirect.com/journal/IMMUNITY/vol/53/issue/2', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/53/issue/1', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/6', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/5', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/4', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/3', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/2', 'https://www.sciencedirect.com/journal/IMMUNITY/vol/52/issue/1']\n",
      "url loading done !\n",
      "start Craw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 99, in article_urls_Craw\n",
      "    reference_dict,title_id = reference_Process(url)\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 118, in reference_Process\n",
      "    print(resp.status_code)\n",
      "AttributeError: 'str' object has no attribute 'status_code'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 99, in article_urls_Craw\n",
      "    reference_dict,title_id = reference_Process(url)\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 118, in reference_Process\n",
      "    print(resp.status_code)\n",
      "AttributeError: 'str' object has no attribute 'status_code'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 99, in article_urls_Craw\n",
      "    reference_dict,title_id = reference_Process(url)\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 118, in reference_Process\n",
      "    print(resp.status_code)\n",
      "AttributeError: 'str' object has no attribute 'status_code'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 99, in article_urls_Craw\n",
      "    reference_dict,title_id = reference_Process(url)\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 118, in reference_Process\n",
      "    print(resp.status_code)\n",
      "AttributeError: 'str' object has no attribute 'status_code'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 99, in article_urls_Craw\n",
      "    reference_dict,title_id = reference_Process(url)\n",
      "  File \"<ipython-input-1-a6cf9176e04e>\", line 118, in reference_Process\n",
      "    print(resp.status_code)\n",
      "AttributeError: 'str' object has no attribute 'status_code'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "fake_ua = UserAgent()\n",
    "headers = {\n",
    "    'user-agent':fake_ua.random,\n",
    "    'cookie': '__cfduid=d813dceed2b4f1201371108fba98bdced1599533490; EUID=c4cc9ca1-6368-4e1a-bb75-fa7572a5f785; has_multiple_organizations=false; id_ab=B:100:8:701c76df-1da8-4a1a-88a2-92513e657af6; utt=f405266b88b64710d9cb904e6534597aaf6dd90; AMCVS_4D6368F454EC41940A4C98A6%40AdobeOrg=1; __gads=ID=aed21115087b2085:T=1599533496:S=ALNI_MbA50dE0pw_wymipQOzAZ9OlBssbw; mboxes=%5B%7B%22index%22%3A0%2C%22name%22%3A%22article-page-view-only-pdf-server-side-mbox%22%7D%5D; usbls=1; fingerPrintToken=97ac2b8cf0e2f1dd3e4ccab725803e63; sd_session_id=ee3ac9602f214445a03b2ca6c93c1c88844bgxrqa; acw=ee3ac9602f214445a03b2ca6c93c1c88844bgxrqa%7C%24%7CB3F0F3F608F8D01C5C1E016B4C654CEE7554AAE072749DAF85D85A7D0A5C216742637C2A224F20C0BD897E56E275B8FCA75652AE48A06CF13FBA44D1BD4E4F2EAFE9C31A29ED2080B6DA1F7CB1786ABB; ANONRA_COOKIE=3BF848533EB21B1AAD335803E309E2FC570004E8A67F04B7CF27F8C8E76F26450E2C12DC06BE0D89B7FBF85D00729F5495FC7ADA35E78036; SD_ART_LINK_STATE=%3Ce%3E%3Cq%3Escience%3C%2Fq%3E%3Corg%3Ejrnl_archive%3C%2Forg%3E%3Cz%3Erslt_list_item%3C%2Fz%3E%3Crdt%3E2020%2F09%2F11%2F01%3A25%3A32%3A609%3C%2Frdt%3E%3Cenc%3EN%3C%2Fenc%3E%3C%2Fe%3E; mbox=session%2373b2a177f9f648f0b377e333286a2d5e%231599792987%7CPC%23111599787533312-97508.34_0%231663035927; MIAMISESSION=6b567d5f-7145-4d2f-b072-54053f66490a:3777255400; SD_REMOTEACCESS=eyJhY2NvdW50SWQiOiI1MzU1MiIsImRlcHRJZCI6Ijc1NjM3IiwidGltZXN0YW1wIjoxNTk5ODAyNjAwOTI1fQ==; s_pers=%20v8%3D1599802606734%7C1694410606734%3B%20v8_s%3DFirst%2520Visit%7C1599804406734%3B%20c19%3Dsd%253Abrowse%253Ajournal%253Aarchive%7C1599804406798%3B%20v68%3D1599802605330%7C1599804406861%3B; AMCV_4D6368F454EC41940A4C98A6%40AdobeOrg=870038026%7CMCIDTS%7C18517%7CMCMID%7C49729833905657570613356864807633985798%7CMCAAMLH-1600407407%7C11%7CMCAAMB-1600407407%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1599809807s%7CNONE%7CMCAID%7CNONE%7CMCCIDH%7C1134298126%7CvVersion%7C5.0.0; s_sess=%20e41%3D1%3B%20s_cpc%3D1%3B%20s_cc%3Dtrue%3B%20s_ppvl%3Dsd%25253Asearch%25253Aresults%25253Acustomer%25253Aanon%252C13%252C13%252C599%252C1280%252C599%252C1280%252C800%252C2%252CP%3B%20s_ppv%3Dsd%25253Aproduct%25253Ajournal%25253Aarticle%252C5%252C5%252C605%252C1280%252C599%252C1280%252C800%252C2%252CP%3B'\n",
    "    }\n",
    "\n",
    "none_file_name = []\n",
    "\n",
    "def read_file_names():\n",
    "    file_names = []\n",
    "    file_name_list = pd.read_csv('/users/heshuwen/desktop/all_journal.csv')['0']\n",
    "    for file_name in file_name_list:\n",
    "        file_name = file_name.replace(' ','-')\n",
    "        file_names.append(file_name)\n",
    "    return file_names\n",
    "\n",
    "def get_volume(file_name):\n",
    "    global none_file_name\n",
    "    try:\n",
    "        print('Loding {} now!'.format(file_name))\n",
    "        dom = requests.get('https://www.sciencedirect.com/journal/{}/issues'.format(file_name),headers = headers).text\n",
    "        soup = BeautifulSoup(dom,'lxml')\n",
    "        ajax = soup.find('div', class_='js-ad-banner').find('script').get_text()\n",
    "        pattern = r\"\"\".*?defineSlot\\('.*?/ISSN(\\d+)'\"\"\"\n",
    "        pattern = re.compile(pattern, re.S)\n",
    "        m = pattern.match(dom).group(1)\n",
    "        volume = 'https://www.sciencedirect.com/journal/{}/year/2020/issues'.format(m)\n",
    "        print(volume)\n",
    "        journal_urls = get_journal_url(file_name,volume)\n",
    "        with open('./Done_File_name.csv','a') as Done_file:\n",
    "            Done_file.write(file_name)\n",
    "            Done_file.close()\n",
    "        return journal_urls\n",
    "    except:\n",
    "        print('{} not in volume!'.format(file_name))\n",
    "        none_file_name.append(file_name)\n",
    "        with open('./none_file_name.csv','a') as f:\n",
    "            f.write(file_name)\n",
    "            f.close()\n",
    "\n",
    "def get_journal_url(file_name,volume):\n",
    "    journal_urls = []\n",
    "    volume_url = requests.get(volume,headers = headers).json()\n",
    "    time.sleep(random.randint(3,4))\n",
    "    for uriLookup in volume_url['data']:\n",
    "        url = 'https://www.sciencedirect.com/journal/{}'.format(file_name) + uriLookup['uriLookup']\n",
    "        journal_urls.append(url)\n",
    "    print(journal_urls)\n",
    "    return journal_urls\n",
    "\n",
    "def get_volume_urls(journal_urls):  # 需要修改\n",
    "    volume_urls = journal_urls \n",
    "    return volume_urls\n",
    "    \n",
    "def get_article_urls(volume_urls):\n",
    "    global headers\n",
    "    article_urls = []\n",
    "    for url in volume_urls:\n",
    "        resp = requests.get(url,headers = headers).text\n",
    "        soup = BeautifulSoup(resp,'lxml')\n",
    "        article_url = ['https://www.sciencedirect.com' + i.attrs['href'] for i in soup.find_all('a',class_=\"anchor article-content-title u-margin-xs-top u-margin-s-bottom\")]\n",
    "        article_urls.extend(article_url)\n",
    "#         time.sleep(random.randint(0,1))\n",
    "    print('url loading done !')\n",
    "    return article_urls\n",
    "\n",
    "def article_urls_Craw(article_urls):\n",
    "    print('start Craw')\n",
    "    global headers\n",
    "    refs = []\n",
    "    for url in article_urls:\n",
    "        try:\n",
    "            resp_dic = {\n",
    "\n",
    "            }\n",
    "            article_url = requests.get(url,headers = headers).text\n",
    "            article_dom = BeautifulSoup(article_url,'lxml')\n",
    "            title_dom = article_dom.find('span',class_=\"title-text\").get_text()\n",
    "            author_list = article_dom.find_all('a',class_=\"author size-m workspace-trigger\")\n",
    "            author_lists = []\n",
    "            for author in author_list:\n",
    "                author = author.find('span',class_=\"content\")\n",
    "                author_xing = author.find('span',class_=\"text given-name\").get_text()\n",
    "                author_name = author.find('span',class_=\"text surname\").get_text()\n",
    "                author_full_name = author_xing + author_name\n",
    "                author_lists.append(author_full_name)\n",
    "            author_dom = ','.join(author_lists)\n",
    "            reference_dict,title_id = reference_Process(url)\n",
    "            resp_dic['title'] = title_dom\n",
    "            resp_dic['author'] = author_lists\n",
    "            resp_dic['title_id'] = title_id\n",
    "            print(title_dom)\n",
    "            print(title_id)\n",
    "            refs.append(resp_dic)\n",
    "            time.sleep(random.randint(1,2))\n",
    "            with open ('/users/heshuwen/desktop/JCR_JOURNAL/every_article_reference/{}.json'.format(title_id),'w') as reference_file:\n",
    "                 json.dump(reference_dict,reference_file)\n",
    "            print('Craw done!')\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    return refs\n",
    "    \n",
    "def reference_Process(article_url):\n",
    "    global headers\n",
    "    resps = requests.get(article_url,headers = headers)\n",
    "    resp = resps.text\n",
    "    print(resp.status_code)\n",
    "    print(article_url)\n",
    "    reference_dom = BeautifulSoup(resp,'lxml')\n",
    "    title_id = reference_dom.find('meta').attrs['content']\n",
    "    token = reference_dom.find('script',type = \"application/json\").get_text()\n",
    "    dom = json.loads(token)\n",
    "    entitledToken = dom['article']['entitledToken']\n",
    "    reference_url = 'https://www.sciencedirect.com/sdfe/arp/pii/' + title_id + '/references?entitledToken=' + entitledToken\n",
    "    reference_dict = json.loads(requests.get(reference_url,headers = headers).text)\n",
    "    return reference_dict,title_id\n",
    "\n",
    "\n",
    "def save_json_file(refs,file_name):\n",
    "    print('start saving_data')\n",
    "    with open ('/users/heshuwen/desktop/JCR_JOURNAL/paper_name/{}.json'.format(file_name),'w') as f:\n",
    "        json.dump(refs,f) \n",
    "    print('saving done')\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    file_names = read_file_names()\n",
    "    for file_name in file_names:\n",
    "        journal_urls = get_volume(file_name)\n",
    "        try:\n",
    "            volume_urls = get_volume_urls(journal_urls)\n",
    "            article_urls = get_article_urls(volume_urls)\n",
    "            refs = article_urls_Craw(article_urls)\n",
    "            save_json_file(refs,file_name)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
